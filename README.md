#Intro To Scraping

# Dev Setup
0. Clone this repo: `git clone git@github.com:EoghanL/IntroToScraping.git`;
1. Create a virtualenv for Python 3.6: `mkvirtualenv scraping-intro -p /usr/local/bin/python3.6`;
2. Copy default postactivate: `cp contrib/proj_postactivate.example ~/.virtualenvs/scraping-intro/bin/postactivate`;
4. Install the packages: `pip install -r dev-requirements.txt`;


# Introduction

As the internet becomes a larger and more complete part of our lives the wealth of data that can be found online grows exponentially.
This leads to many opportunities where scraping the web can lead to more well informed stock market decisions, an immediate notification of
a price drop on certain items, or even exact identification of minor changes to a webpage. While our upcoming project may only need one of these use
cases learning scraping can certainly open new solutions to future problems you may encounter.

# Types of Scraping

Wikipedia lists the different types of scraping as:

  **Human copy-and-paste**
  Sometimes even the best web-scraping technology cannot replace a human’s manual examination and copy-and-paste, and sometimes this may be the only workable
  solution when the websites for scraping explicitly set up barriers to prevent machine automation.

  **Text pattern matching**
  A simple yet powerful approach to extract information from web pages can be based on the UNIX grep command or regular expression-matching facilities of programming languages (for instance Perl or Python).

  **HTTP programming**
  Static and dynamic web pages can be retrieved by posting HTTP requests to the remote web server using socket programming.

  **HTML parsing**
  Many websites have large collections of pages generated dynamically from an underlying structured source like a database.
  Data of the same category are typically encoded into similar pages by a common script or template.
  In data mining, a program that detects such templates in a particular information source, extracts its content and translates it into a relational form, is called a wrapper. 
  Wrapper generation algorithms assume that input pages of a wrapper induction system conform to a common template and that they can be easily identified in terms of a URL common scheme.
  Moreover, some semi-structured data query languages, such as XQuery and the HTQL, can be used to parse HTML pages and to retrieve and transform page content.

  **DOM parsing**
  By embedding a full-fledged web browser, such as the Internet Explorer or the Mozilla browser control, programs can retrieve the dynamic content generated by client-side scripts.
  These browser controls also parse web pages into a DOM tree, based on which programs can retrieve parts of the pages.

  Additionally, many dynamic websites often expose their data in internal JavaScript variables.
  Therefore, using a JavaScript-enabled web browser it is often possible to extract required data simply by accessing variables stored, for example, on the Window object.

  **Vertical aggregation**
  There are several companies that have developed vertical specific harvesting platforms.
  These platforms create and monitor a multitude of “bots” for specific verticals with no "man in the loop" (no direct human involvement), and no work related to a specific target site.
  The preparation involves establishing the knowledge base for the entire vertical and then the platform creates the bots automatically.
  The platform's robustness is measured by the quality of the information it retrieves (usually number of fields)
  and its scalability (how quick it can scale up to hundreds or thousands of sites). This scalability is mostly used to target the Long Tail of sites that common aggregators find complicated or too labor-intensive to harvest content from.

  **Semantic annotation recognizing**
  The pages being scraped may embrace metadata or semantic markups and annotations, which can be used to locate specific data snippets.
  If the annotations are embedded in the pages, as Microformat does, this technique can be viewed as a special case of DOM parsing.
  In another case, the annotations, organized into a semantic layer, are stored and managed separately from the web pages, so the scrapers can retrieve data schema and instructions from this layer before scraping the pages.

  **Computer vision web-page analysis**
  There are efforts using machine learning and computer vision that attempt to identify and extract information from web pages by interpreting pages visually as a human being might.

  That's a pretty long list of types of scrapers to choose from, luckily we'll be focusing on three types of scraping for this tutorial.

# What we will be using

**Raw API Requests**

The first, and arguably simplest, form of web scraping that we use involves directly querying the API that serves the web page the data it uses and parsing through that to get what we need.
These requests usually come in two main formats:

  1.) This type of request has the query params dictate how the response will be sent back.
      You can manually alter things like latitude, longitude, search radius, and max returned results.
      Changing these can reduce the number of requests you need to make or give you more accurate results to name a couple of outcomes.
      We can see an example of this type of request here - https://www.rei.com/rest/stores?retail=true&dist=7500&limit=1500&visible=true&lat=39.82832&long=-98.5795'
      Feel free to drop the above link directly into your address bar and try messing around with the query params. See what kind of different responses you get!
      The full scraper can be found in `./scraper_examples/rei_scraper.py`

  2.) The second type of request that falls into this category uses form data and headers in place of query params and requires a bit more work to implement.
      First, the network tab must be used to find a request made to the API you are trying to query.
      From there you can check grab the url, request headers, and form data.
      Lastly, you must use the `requests` library to structure your request in a way that will be accepted by the API.
      Sounds tedious, luckily the Postman app has made this process a breeze. Check the screencast below to see a video example of how this works.

**Parsing Content with BeautifulSoup**

The second type of scraping is done by loading either the entire HTML webpage and parsing it or requesting the pre-rendered dynamic template that then loads in the data using scripts.
Regardless of which way we are using this method we rely on the BeautifulSoup library to pull the data out of the response.

  1.) The first, and slightly trickier type of request to parse is one that uses scripts.
      The data is grabbed by using the `requests` library to load the response and then parse the information we need out of the script.
      An example of this can be seen in `./scraper_examples/hardees_scraper`
      https://maps.ckr.com/stores/search?brand=hardees&q=11104&brand_id=1

  2.) The second is where a static HTML page is loaded and we want to parse data from it.
      This functions similarly to the above example but is easier to implement due to the fact that we don't have to do as much string manipulation to get the data we want.
      Rather than looking for script tags we convert the response to a BeautifulSoup object which then allows us to query it for specific elements.
      An example of this can be seen in `./scraper_examples/dq_scraper`

  Notes: You've probably noticed that these two scrapers are not that different from each other in their implementation.
         This can be misleading because the processes for extracting the data we need are quite distinct.
         Check the respective scrapers for more in-depth comments on what is happening where and why.

**Selenium**


TODO: add comments to the scrapers on functionality.
